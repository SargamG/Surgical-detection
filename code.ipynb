{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCBdJVsbvup"
      },
      "source": [
        "## Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0x_p0-zbPLy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as basemodels\n",
        "import platform\n",
        "import ivtmetrics\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IU0oUh7GD3u"
      },
      "outputs": [],
      "source": [
        "class T50(Dataset):\n",
        "    def __init__(self, img_dir, triplet_file, tool_file, verb_file, target_file, phase_file, transform=None, target_transform=None):\n",
        "        self.triplet_labels = np.loadtxt(triplet_file, dtype=int, delimiter=',')\n",
        "        self.tool_labels = np.loadtxt(tool_file, dtype=int, delimiter=',')\n",
        "        self.verb_labels = np.loadtxt(verb_file, dtype=int, delimiter=',')\n",
        "        self.target_labels = np.loadtxt(target_file, dtype=int, delimiter=',')\n",
        "        self.phase_labels = np.loadtxt(phase_file, dtype=int, delimiter=',')\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplet_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        triplet_label = self.triplet_labels[index, 1:]\n",
        "        tool_label = self.tool_labels[index, 1:]\n",
        "        verb_label = self.verb_labels[index, 1:]\n",
        "        target_label = self.target_labels[index, 1:]\n",
        "        phase_label = self.phase_labels[index, 1:]\n",
        "        basename = \"{}.png\".format(str(self.triplet_labels[index, 0]).zfill(6))\n",
        "        img_path = os.path.join(self.img_dir, basename)\n",
        "        image    = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            triplet_label = self.target_transform(triplet_label)\n",
        "        return image, (tool_label, verb_label, target_label, triplet_label, phase_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUsIb3hEd6B8"
      },
      "outputs": [],
      "source": [
        "class CholecT50():\n",
        "    def __init__(self,\n",
        "                dataset_dir):\n",
        "      self.dataset_dir = dataset_dir\n",
        "      train_videos = [1, 2, 4, 5, 6, 8, 10, 12, 13, 14]\n",
        "      test_videos = [92, 96, 103, 110, 111]\n",
        "      self.train_records = ['VID{}'.format(str(v).zfill(2)) for v in train_videos]\n",
        "      self.test_records  = ['VID{}'.format(str(v).zfill(2)) for v in test_videos]\n",
        "      trainform, testform = self.transform()\n",
        "      self.build_train_dataset(trainform)\n",
        "      self.build_test_dataset(testform)\n",
        "\n",
        "    def no_augumentation(self, x):\n",
        "      return x\n",
        "\n",
        "    def transform(self):\n",
        "      normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "      op_test   = [transforms.Resize((256, 448)), transforms.ToTensor(), normalize]\n",
        "      op_train  = [transforms.Resize((256, 448)), transforms.ToTensor(), normalize]\n",
        "      testform  = transforms.Compose(op_test)\n",
        "      trainform = transforms.Compose(op_train)\n",
        "      return trainform, testform\n",
        "\n",
        "    def build_train_dataset(self, transform):\n",
        "        iterable_dataset = []\n",
        "        for video in self.train_records:\n",
        "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'data', video),\n",
        "                        triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video)),\n",
        "                        tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video)),\n",
        "                        verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video)),\n",
        "                        target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video)),\n",
        "                        phase_file =  os.path.join(self.dataset_dir, 'phase', '{}.txt'.format(video)),\n",
        "                        transform=transform)\n",
        "            iterable_dataset.append(dataset)\n",
        "        self.train_dataset = iterable_dataset\n",
        "\n",
        "    def build_test_dataset(self, transform):\n",
        "        iterable_dataset = []\n",
        "        for video in self.test_records:\n",
        "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'data', video),\n",
        "                triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video)),\n",
        "                tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video)),\n",
        "                verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video)),\n",
        "                target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video)),\n",
        "                phase_file = os.path.join(self.dataset_dir, 'phase', '{}.txt'.format(video)),\n",
        "                transform=transform)\n",
        "            iterable_dataset.append(dataset)\n",
        "        self.test_dataset = iterable_dataset\n",
        "\n",
        "    def build(self):\n",
        "        return (self.train_dataset, self.test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDnYHq6FHqHb"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_iDFcr5H6vP"
      },
      "outputs": [],
      "source": [
        "OUT_HEIGHT = 8\n",
        "OUT_WIDTH  = 14"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBvTWh8BH9oI"
      },
      "outputs": [],
      "source": [
        "# Feature extraction backbone\n",
        "class BaseModel(nn.Module):\n",
        "    def __init__(self, hr_output=False, *args):\n",
        "        super(BaseModel, self).__init__(*args)\n",
        "        self.output_feature = {}\n",
        "        self.basemodel = basemodels.resnet18(pretrained=True)\n",
        "        if hr_output: self.increase_resolution()\n",
        "        self.basemodel.layer1[1].bn2.register_forward_hook(self.get_activation('low_level_feature'))\n",
        "        self.basemodel.layer4[1].bn2.register_forward_hook(self.get_activation('high_level_feature'))\n",
        "\n",
        "    def increase_resolution(self):\n",
        "        global OUT_HEIGHT, OUT_WIDTH\n",
        "        self.basemodel.layer3[0].conv1.stride = (1,1)\n",
        "        self.basemodel.layer3[0].downsample[0].stride=(1,1)\n",
        "        self.basemodel.layer4[0].conv1.stride = (1,1)\n",
        "        self.basemodel.layer4[0].downsample[0].stride=(1,1)\n",
        "        OUT_HEIGHT *= 4\n",
        "        OUT_WIDTH  *= 4\n",
        "        print(\"using high resolution output ({}x{})\".format(OUT_HEIGHT,OUT_WIDTH))\n",
        "\n",
        "\n",
        "    def get_activation(self, layer_name):\n",
        "        def hook(module, input, output):\n",
        "            self.output_feature[layer_name] = output\n",
        "        return hook\n",
        "\n",
        "    def forward(self, x):\n",
        "        _ = self.basemodel(x)\n",
        "        return self.output_feature['high_level_feature'], self.output_feature['low_level_feature']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PXiTCgNLlOi"
      },
      "outputs": [],
      "source": [
        "# Tool Weakly-Supervised localization\n",
        "class Tool_WSL(nn.Module):\n",
        "    def __init__(self, num_class, depth=64):\n",
        "        super(Tool_WSL, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=512, out_channels=depth, kernel_size=3, padding=1)\n",
        "        self.cam   = nn.Conv2d(in_channels=depth, out_channels=num_class, kernel_size=1)\n",
        "        self.elu   = nn.ELU()\n",
        "        self.bn    = nn.BatchNorm2d(depth)\n",
        "        self.gmp   = nn.AdaptiveMaxPool2d((1,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.conv1(x)\n",
        "        feature = self.bn(feature)\n",
        "        feature = self.elu(feature)\n",
        "        cam     = self.cam(feature)\n",
        "        logits  = self.gmp(cam).squeeze(-1).squeeze(-1)\n",
        "        return cam, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZdftd3i0BZd"
      },
      "outputs": [],
      "source": [
        "class ConvLSTMCell(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size):\n",
        "        super(ConvLSTMCell, self).__init__()\n",
        "        padding = kernel_size // 2  # to ensure the same spatial dimensions\n",
        "\n",
        "        self.input_channels = input_channels\n",
        "        self.hidden_channels = hidden_channels\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        self.conv = nn.Conv2d(input_channels + hidden_channels,\n",
        "                              4 * hidden_channels,\n",
        "                              kernel_size,\n",
        "                              padding=padding)\n",
        "\n",
        "    def forward(self, x, h_cur, c_cur):\n",
        "        combined = torch.cat([x, h_cur], dim=1)  # concatenate along the channel axis\n",
        "        conv_output = self.conv(combined)\n",
        "        (cc_i, cc_f, cc_o, cc_g) = torch.split(conv_output, self.hidden_channels, dim=1)\n",
        "\n",
        "        i = torch.sigmoid(cc_i)\n",
        "        f = torch.sigmoid(cc_f)\n",
        "        o = torch.sigmoid(cc_o)\n",
        "        g = torch.tanh(cc_g)\n",
        "\n",
        "        c_next = f * c_cur + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "\n",
        "        return h_next, c_next\n",
        "\n",
        "class ConvLSTM2d(nn.Module):\n",
        "    def __init__(self, input_channels, hidden_channels, kernel_size, num_layers, batch_first=False):\n",
        "        super(ConvLSTM2d, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_first = batch_first\n",
        "\n",
        "        self.cells = nn.ModuleList()\n",
        "        self.cells.append(ConvLSTMCell(input_channels, hidden_channels, kernel_size))\n",
        "        for _ in range(1, num_layers):\n",
        "            self.cells.append(ConvLSTMCell(hidden_channels, hidden_channels, kernel_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.batch_first:\n",
        "            x = x.permute(1, 0, 2, 3, 4)  # Change to (seq_len, batch, channels, height, width)\n",
        "\n",
        "        _, b, _, h, w = x.shape\n",
        "        h_t, c_t = [], []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            h_t.append(torch.zeros(b, self.cells[i].hidden_channels, h, w, device=x.device))\n",
        "            c_t.append(torch.zeros(b, self.cells[i].hidden_channels, h, w, device=x.device))\n",
        "\n",
        "        output_inner = []\n",
        "        for t in range(x.size(0)):\n",
        "            x_t = x[t, :, :, :, :]\n",
        "            for i in range(self.num_layers):\n",
        "                h_t[i], c_t[i] = self.cells[i](x_t, h_t[i], c_t[i])\n",
        "                x_t = h_t[i]\n",
        "            output_inner.append(h_t[-1])\n",
        "\n",
        "        output_inner = torch.stack(output_inner, dim=0)\n",
        "\n",
        "        if self.batch_first:\n",
        "            output_inner = output_inner.permute(1, 0, 2, 3, 4)  # Back to (batch, seq_len, channels, height, width)\n",
        "\n",
        "        return output_inner, (h_t[-1], c_t[-1])\n",
        "\n",
        "class LSTMblock(nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super(LSTMblock, self).__init__()\n",
        "        self.conv_lstm = ConvLSTM2d(input_channels=512, hidden_channels=hidden_channels, kernel_size=3, num_layers=2, batch_first=True)\n",
        "        self.bn = nn.BatchNorm2d(hidden_channels)\n",
        "        self.activation = nn.ELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        lstm_out, _ = self.conv_lstm(x)\n",
        "        lstm_out = lstm_out[:, -1, :, :, :]  # Take the output of the last time step i.e the present time step\n",
        "        lstm_out = self.bn(lstm_out)\n",
        "        lstm_out = self.activation(lstm_out)\n",
        "        return lstm_out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3gXN2XTT0Um"
      },
      "outputs": [],
      "source": [
        "# Phase Weakly-Supervised localization\n",
        "class Phase_WSL(nn.Module):\n",
        "    def __init__(self, num_phase, in_channels, depth=64):\n",
        "        super(Phase_WSL, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=depth, kernel_size=3, padding=1)\n",
        "        self.cam   = nn.Conv2d(in_channels=depth, out_channels=num_phase, kernel_size=1)\n",
        "        self.verb_phase_cam= nn.Conv2d(in_channels=num_phase, out_channels=10, kernel_size=1)\n",
        "        self.triplet_cam= nn.Conv2d(in_channels=depth, out_channels=100, kernel_size=1)\n",
        "        self.elu   = nn.ELU()\n",
        "        self.bn1    = nn.BatchNorm2d(depth)\n",
        "        self.bn2    = nn.BatchNorm2d(num_phase)\n",
        "        self.bn3    = nn.BatchNorm2d(100)\n",
        "        self.gmp   = nn.AdaptiveMaxPool2d((1,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature = self.conv1(x)\n",
        "        feature = self.bn1(feature)\n",
        "        feature = self.elu(feature)\n",
        "        triplet_cam = self.triplet_cam(feature)\n",
        "        triplet_cam = self.bn3(triplet_cam)\n",
        "        triplet_cam = self.elu(triplet_cam)\n",
        "        cam     = self.cam(feature)\n",
        "        logits_expanded  = self.gmp(cam)\n",
        "        logits  = logits_expanded.squeeze(-1).squeeze(-1)\n",
        "        cam     = self.bn2(cam)\n",
        "        cam     = self.elu(cam)\n",
        "        cam     = cam * logits_expanded\n",
        "        verb_phase_cam=self.verb_phase_cam(cam)\n",
        "        return triplet_cam, logits, verb_phase_cam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V1sKXYsNFC2"
      },
      "outputs": [],
      "source": [
        "# Class Activation Guided Attention Mechanism\n",
        "class CAGAM(nn.Module):\n",
        "    def __init__(self, num_tool, num_verb, num_target, in_depth=512):\n",
        "        super(CAGAM, self).__init__()\n",
        "        out_depth               = num_tool\n",
        "        self.verb_context       = nn.Conv2d(in_channels=in_depth, out_channels=out_depth, kernel_size=3, padding=1)\n",
        "        self.verb_query         = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.verb_tool_query    = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.verb_key           = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.verb_tool_key      = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.verb_tool_cmap     = nn.Conv2d(in_channels=out_depth, out_channels=num_verb, kernel_size=1)\n",
        "        self.verb_cmap          = nn.Conv2d(in_channels=num_verb, out_channels=num_verb, kernel_size=1)\n",
        "        self.target_context     = nn.Conv2d(in_channels=in_depth, out_channels=out_depth, kernel_size=3, padding=1)\n",
        "        self.target_query       = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.target_tool_query  = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.target_key         = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.target_tool_key    = nn.Conv2d(in_channels=out_depth, out_channels=out_depth, kernel_size=1)\n",
        "        self.target_cmap        = nn.Conv2d(in_channels=out_depth, out_channels=num_target, kernel_size=1)\n",
        "        self.gmp       = nn.AdaptiveMaxPool2d((1,1))\n",
        "        self.elu       = nn.ELU()\n",
        "        self.soft      = nn.Softmax(dim=1)\n",
        "        self.flat      = nn.Flatten(2,3)\n",
        "        self.bn1       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn2       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn3       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn4       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn5       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn6       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn7       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn8       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn9       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn10      = nn.BatchNorm2d(out_depth)\n",
        "        self.bn11      = nn.BatchNorm2d(out_depth)\n",
        "        self.bn12      = nn.BatchNorm2d(out_depth)\n",
        "        self.bn13      = nn.BatchNorm2d(num_verb)\n",
        "        self.encoder_cagam_verb_beta   = torch.nn.Parameter(torch.randn(1))\n",
        "        self.encoder_cagam_target_beta = torch.nn.Parameter(torch.randn(1))\n",
        "\n",
        "    def get_verb(self, raw, cam, phase_cam):\n",
        "        x  = self.elu(self.bn1(self.verb_context(raw)))\n",
        "        z  = x.clone()\n",
        "        sh = list(z.shape)\n",
        "        sh[0] = -1\n",
        "        q1 = self.elu(self.bn2(self.verb_query(x)))\n",
        "        k1 = self.elu(self.bn3(self.verb_key(x)))\n",
        "        w1 = self.flat(k1).matmul(self.flat(q1).transpose(-1,-2))\n",
        "        q2 = self.elu(self.bn4(self.verb_tool_query(cam)))\n",
        "        k2 = self.elu(self.bn5(self.verb_tool_key(cam)))\n",
        "        w2 = self.flat(k2).matmul(self.flat(q2).transpose(-1,-2))\n",
        "        attention = (w1 * w2) / torch.sqrt(torch.tensor(sh[-1], dtype=torch.float32))\n",
        "        attention = self.soft(attention)\n",
        "        v = self.flat(z)\n",
        "        e = (attention.matmul(v) * self.encoder_cagam_verb_beta).reshape(sh)\n",
        "        e = self.bn6(e + z)\n",
        "        #e = e * phase_cam# If error occurs try printing shapes of e and phase_cam\n",
        "        verb_tool_cmap = self.verb_tool_cmap(e)\n",
        "        f = verb_tool_cmap * phase_cam\n",
        "        f = self.bn13(f)\n",
        "        f = self.elu(f)\n",
        "        cmap = self.verb_cmap(f)\n",
        "        y = self.gmp(cmap).squeeze(-1).squeeze(-1)\n",
        "        return cmap, y\n",
        "\n",
        "    def get_target(self, raw, cam):\n",
        "        x  = self.elu(self.bn7(self.target_context(raw)))\n",
        "        z  = x.clone()\n",
        "        sh = list(z.shape)\n",
        "        sh[0] = -1\n",
        "        q1 = self.elu(self.bn8(self.target_query(x)))\n",
        "        k1 = self.elu(self.bn9(self.target_key(x)))\n",
        "        w1 = self.flat(k1).transpose(-1,-2).matmul(self.flat(q1))\n",
        "        q2 = self.elu(self.bn10(self.target_tool_query(cam)))\n",
        "        k2 = self.elu(self.bn11(self.target_tool_key(cam)))\n",
        "        w2 = self.flat(k2).transpose(-1,-2).matmul(self.flat(q2))\n",
        "        attention = (w1 * w2) / torch.sqrt(torch.tensor(sh[-1], dtype=torch.float32))\n",
        "        attention = self.soft(attention)\n",
        "        v = self.flat(z)\n",
        "        e = (v.matmul(attention) * self.encoder_cagam_target_beta).reshape(sh)\n",
        "        e = self.bn12(e + z)\n",
        "        cmap = self.target_cmap(e)\n",
        "        y = self.gmp(cmap).squeeze(-1).squeeze(-1)\n",
        "        return cmap, y\n",
        "\n",
        "    def forward(self, x, cam, phase_cam):\n",
        "        cam_v, logit_v = self.get_verb(x, cam, phase_cam)\n",
        "        cam_t, logit_t = self.get_target(x, cam)\n",
        "        return (cam_v, logit_v), (cam_t, logit_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MzGSHfnXcmqb"
      },
      "outputs": [],
      "source": [
        "# Projection function\n",
        "class Projection(nn.Module):\n",
        "    def __init__(self, num_tool=6, num_verb=10, num_target=15, num_class=100, out_depth=128):\n",
        "        super(Projection, self).__init__()\n",
        "        self.triplet_value = nn.Conv2d(in_channels=num_class, out_channels=out_depth, kernel_size=1)\n",
        "        self.i_value   = nn.Conv2d(in_channels=num_tool, out_channels=out_depth, kernel_size=1)\n",
        "        self.v_value   = nn.Conv2d(in_channels=num_verb, out_channels=out_depth, kernel_size=1)\n",
        "        self.t_value   = nn.Conv2d(in_channels=num_target, out_channels=out_depth, kernel_size=1)\n",
        "        self.triplet_query = nn.Linear(in_features=num_class, out_features=out_depth)\n",
        "        self.dropout   = nn.Dropout(p=0.3)\n",
        "        self.triplet_key   = nn.Linear(in_features=num_class, out_features=out_depth)\n",
        "        self.i_key     = nn.Linear(in_features=num_tool, out_features=out_depth)\n",
        "        self.v_key     = nn.Linear(in_features=num_verb, out_features=out_depth)\n",
        "        self.t_key     = nn.Linear(in_features=num_target, out_features=out_depth)\n",
        "        self.gap       = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.elu       = nn.ELU()\n",
        "        self.bn1       = nn.BatchNorm1d(out_depth)\n",
        "        self.bn2       = nn.BatchNorm1d(out_depth)\n",
        "        self.bn3       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn4       = nn.BatchNorm1d(out_depth)\n",
        "        self.bn5       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn6       = nn.BatchNorm1d(out_depth)\n",
        "        self.bn7       = nn.BatchNorm2d(out_depth)\n",
        "        self.bn8       = nn.BatchNorm1d(out_depth)\n",
        "        self.bn9       = nn.BatchNorm2d(out_depth)\n",
        "\n",
        "    def forward(self, cam_i, cam_v, cam_t, X):\n",
        "        q = self.elu(self.bn1(self.triplet_query(self.dropout(self.gap(X).squeeze(-1).squeeze(-1)))))\n",
        "        k = self.elu(self.bn2(self.triplet_key(self.gap(X).squeeze(-1).squeeze(-1))) )\n",
        "        v = self.bn3(self.triplet_value(X))\n",
        "        k1 = self.elu(self.bn4(self.i_key(self.gap(cam_i).squeeze(-1).squeeze(-1))) )\n",
        "        v1 = self.elu(self.bn5(self.i_value(cam_i)) )\n",
        "        k2 = self.elu(self.bn6(self.v_key(self.gap(cam_v).squeeze(-1).squeeze(-1))))\n",
        "        v2 = self.elu(self.bn7(self.v_value(cam_v)) )\n",
        "        k3 = self.elu(self.bn8(self.t_key(self.gap(cam_t).squeeze(-1).squeeze(-1))))\n",
        "        v3 = self.elu(self.bn9(self.t_value(cam_t)))\n",
        "        sh = list(v1.shape)\n",
        "        v  = self.elu(F.interpolate(v, (sh[2],sh[3])))\n",
        "        X  = self.elu(F.interpolate(X, (sh[2],sh[3])))\n",
        "        return (X, (k1,v1), (k2,v2), (k3,v3), (q,k,v))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "inGR1Wbjcvtl"
      },
      "outputs": [],
      "source": [
        "# Multi-head of self and cross attention\n",
        "class MHMA(nn.Module):\n",
        "    def __init__(self, depth, num_class=100, num_heads=4, use_ln=False):\n",
        "        super(MHMA, self).__init__()\n",
        "        self.concat = nn.Conv2d(in_channels=depth*num_heads, out_channels=num_class, kernel_size=3, padding=1)\n",
        "        self.bn     = nn.BatchNorm2d(num_class)\n",
        "        self.ln     = nn.LayerNorm([num_class, OUT_HEIGHT, OUT_WIDTH]) if use_ln else nn.BatchNorm2d(num_class)\n",
        "        self.elu    = nn.ELU()\n",
        "        self.soft   = nn.Softmax(dim=1)\n",
        "        self.heads  = num_heads\n",
        "\n",
        "    def scale_dot_product(self, key, value, query):\n",
        "        dk        = torch.sqrt(torch.tensor(list(key.shape)[-2], dtype=torch.float32))\n",
        "        affinity  = key.matmul(query.transpose(-1,-2))\n",
        "        attn_w    = affinity / dk\n",
        "        attn_w    = self.soft(attn_w)\n",
        "        attention = attn_w.matmul(value)\n",
        "        return attention\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        (X, (k1,v1), (k2,v2), (k3,v3), (q,k,v)) = inputs\n",
        "        query = torch.stack([q]*self.heads, dim=1) # [B,Head,D]\n",
        "        query = query.unsqueeze(dim=-1) # [B,Head,D,1]\n",
        "        key   = torch.stack([k,k1,k2,k3], dim=1) # [B,Head,D]\n",
        "        key   = key.unsqueeze(dim=-1) # [B,Head,D,1]\n",
        "        value = torch.stack([v,v1,v2,v3], dim=1) # [B,Head,D,H,W]\n",
        "        dims  = list(value.shape) # [B,Head,D,H,W]\n",
        "        value = value.reshape([-1,dims[1],dims[2],dims[3]*dims[4]])# [B,Head,D,HW]\n",
        "        attn  = self.scale_dot_product(key, value, query)  # [B,Head,D,HW]\n",
        "        attn  = attn.reshape([-1,dims[1]*dims[2],dims[3],dims[4]]) # [B,DHead,H,W]\n",
        "        mha   = self.elu(self.bn(self.concat(attn)))\n",
        "        mha   = self.ln(mha + X.clone())\n",
        "        return mha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVKzRvscc0q2"
      },
      "outputs": [],
      "source": [
        "# Feed-forward layer\n",
        "class FFN(nn.Module):\n",
        "    def __init__(self, k, num_class=100, use_ln=False):\n",
        "        super(FFN, self).__init__()\n",
        "        def Ignore(x): return x\n",
        "        self.conv1 = nn.Conv2d(in_channels=num_class, out_channels=num_class, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=num_class, out_channels=num_class, kernel_size=1)\n",
        "        self.elu1  = nn.ELU()\n",
        "        self.elu2  = nn.ELU() if k>0 else Ignore\n",
        "        self.bn1   = nn.BatchNorm2d(num_class)\n",
        "        self.bn2   = nn.BatchNorm2d(num_class)\n",
        "        self.ln    = nn.LayerNorm([num_class, OUT_HEIGHT, OUT_WIDTH]) if use_ln else nn.BatchNorm2d(num_class)\n",
        "\n",
        "    def forward(self, inputs,):\n",
        "        x  = self.elu1(self.bn1(self.conv1(inputs)))\n",
        "        x  = self.elu2(self.bn2(self.conv2(x)))\n",
        "        x  = self.ln(x + inputs.clone())\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UglKUoTyc4cw"
      },
      "outputs": [],
      "source": [
        "# Classification layer\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, layer_size, num_class=100):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.gmp = nn.AdaptiveMaxPool2d((1,1))\n",
        "        self.mlp = nn.Linear(in_features=num_class, out_features=num_class)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.gmp(inputs).squeeze(-1).squeeze(-1)\n",
        "        y = self.mlp(x)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeOKA-mooQU7"
      },
      "outputs": [],
      "source": [
        "# Triplet Components Feature Encoder\n",
        "def create_sliding_window_batch(data, window_size=10):\n",
        "    batch_size, channels, height, width = data.shape\n",
        "\n",
        "    # Ensure the window size is valid\n",
        "    if window_size > batch_size:\n",
        "        raise ValueError(\"Window size must be less than or equal to the batch size\")\n",
        "\n",
        "    # List to store sliding window batches\n",
        "    sliding_window_batches = []\n",
        "\n",
        "    for i in range(window_size-1,batch_size):\n",
        "        window = data[i-window_size+1:i+1]  # Extract a window of 'window_size' frames\n",
        "        sliding_window_batches.append(window)\n",
        "\n",
        "    # Stack all windows to create the final tensor\n",
        "    sliding_window_tensor = torch.stack(sliding_window_batches, dim=0)  # Shape: (new_batch_size, window_size, channels, height, width)\n",
        "\n",
        "    return sliding_window_tensor\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, basename='resnet18', num_tool=6,  num_verb=10, num_target=15, num_triplet=100, hr_output=False):\n",
        "        super(Encoder, self).__init__()\n",
        "        depth = 64 if basename == 'resnet18' else 128\n",
        "        self.basemodel  = BaseModel(hr_output)\n",
        "        self.tool_wsl   = Tool_WSL(num_tool, depth)\n",
        "        self.lstm       = LSTMblock(hidden_channels=128)\n",
        "        self.phase_wsl  = Phase_WSL(num_phase=7, in_channels=128)\n",
        "        self.cagam      = CAGAM(num_tool, num_verb, num_target)\n",
        "\n",
        "    def forward(self, x):\n",
        "        high_x, low_x = self.basemodel(x)\n",
        "        lstm_input    = create_sliding_window_batch(high_x)\n",
        "        high_x        = high_x[9:]# Making sure input size is same in all model(batch_size-9,channels,height,width)\n",
        "        enc_i         = self.tool_wsl(high_x)\n",
        "        lstm_feature  = self.lstm(lstm_input)\n",
        "        enc_ivt, phase_logits, verb_phase_cam = self.phase_wsl(lstm_feature)\n",
        "        enc_v, enc_t  = self.cagam(high_x, enc_i[0], verb_phase_cam)\n",
        "        return enc_i, enc_v, enc_t, enc_ivt, phase_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iair2n5VrXYv"
      },
      "outputs": [],
      "source": [
        "# MultiHead Attention Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, layer_size, d_model, num_heads, num_class=100, use_ln=False):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.projection = nn.ModuleList([Projection(num_class=num_class, out_depth=d_model) for i in range(layer_size)])\n",
        "        self.mhma       = nn.ModuleList([MHMA(num_class=num_class, depth=d_model, num_heads=num_heads, use_ln=use_ln) for i in range(layer_size)])\n",
        "        self.ffnet      = nn.ModuleList([FFN(k=layer_size-i-1, num_class=num_class, use_ln=use_ln) for i in range(layer_size)])\n",
        "        self.classifier = Classifier(num_class)\n",
        "\n",
        "    def forward(self, enc_i, enc_v, enc_t, enc_ivt):\n",
        "        X = enc_ivt.clone()\n",
        "        for P, M, F in zip(self.projection, self.mhma, self.ffnet):\n",
        "            X = P(enc_i[0], enc_v[0], enc_t[0], X)\n",
        "            X = M(X)\n",
        "            X = F(X)\n",
        "        logits = self.classifier(X)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvQbWkFWsr9l"
      },
      "outputs": [],
      "source": [
        "# Model Rendezvous\n",
        "class Rendezvous(nn.Module):\n",
        "    def __init__(self, basename=\"resnet18\", num_tool=6, num_verb=10, num_target=15, num_triplet=100, layer_size=2, num_heads=4, d_model=128, hr_output=False, use_ln=False):\n",
        "        super(Rendezvous, self).__init__()\n",
        "        self.encoder = Encoder(basename, num_tool, num_verb, num_target, num_triplet, hr_output=hr_output)\n",
        "        self.decoder = Decoder(layer_size, d_model, num_heads, num_triplet, use_ln=use_ln)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        enc_i, enc_v, enc_t, enc_ivt, phase_logits = self.encoder(inputs)\n",
        "        dec_ivt = self.decoder(enc_i, enc_v, enc_t, enc_ivt)\n",
        "        return enc_i, enc_v, enc_t, dec_ivt, phase_logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCsFBCWy03NK"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfpT-xi905NQ"
      },
      "outputs": [],
      "source": [
        "!pip install ivtmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqExIjCMPAnx"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_video_wise_loop(dataloader, model, optimizer, scheduler, loss_fn_p, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt, activation, final_eval=False):\n",
        "    model.train()\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    for batch, (img, (y1, y2, y3, y4, y5)) in enumerate(dataloader):\n",
        "\n",
        "        if (img.shape[0]<10):\n",
        "            continue\n",
        "        y1 = y1[9:]\n",
        "        y2 = y2[9:]\n",
        "        y3 = y3[9:]\n",
        "        y4 = y4[9:]\n",
        "        y5 = y5[9:]\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (LSTM model for video-wise processing)\n",
        "        tool, verb, target, triplet, phase = model(img)\n",
        "\n",
        "        cam_i, logit_i  = tool\n",
        "        cam_v, logit_v  = verb\n",
        "        cam_t, logit_t  = target\n",
        "        logit_ivt       = triplet\n",
        "        logit_phase     = phase\n",
        "\n",
        "        # Compute loss for each output\n",
        "        loss_tool = loss_fn_i(logit_i, y1.float())\n",
        "        loss_verb = loss_fn_v(logit_v, y2.float())\n",
        "        loss_target = loss_fn_t(logit_t, y3.float())\n",
        "        loss_triplet = loss_fn_ivt(logit_ivt, y4.float())\n",
        "        loss_phase = loss_fn_p(logit_phase, y5.float())\n",
        "\n",
        "        # Combine the losses (you can weight the losses based on your need)\n",
        "        total_loss = loss_tool + loss_verb + loss_target + loss_triplet + loss_phase\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Optionally: log metrics or losses for monitoring\n",
        "        if ((batch % 10 == 0) or (batch == num_batches-1)):  # Log every 10 batches\n",
        "            print(f\"Batch {batch+1}/{num_batches}, Loss: {total_loss.item()}, Tool loss: {loss_tool.item()}, Verb loss: {loss_verb.item()}, Target loss: {loss_target.item()}, Triplet loss: {loss_triplet.item()}, Phase loss: {loss_phase.item()}\")\n",
        "\n",
        "# Train for each video-wise batch\n",
        "def train_video_wise(dataloader, model, optimizer, activation, scheduler, loss_fn_p, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt):\n",
        "    for video_dataloader in dataloader:\n",
        "        train_video_wise_loop(video_dataloader, model, optimizer, scheduler, loss_fn_p, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt, activation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxCkd55YlnSU"
      },
      "outputs": [],
      "source": [
        "dataset = CholecT50(dataset_dir=\"/content/drive/MyDrive/CholecT50_unzipped/CholecT50\")\n",
        "\n",
        "# build dataset\n",
        "train_dataset, test_dataset = dataset.build()\n",
        "\n",
        "def get_default_device():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def to_device(data,device):\n",
        "  if isinstance(data,(list,tuple)):\n",
        "    return [to_device(x,device) for x in data]\n",
        "  else:\n",
        "    return data.to(device,non_blocking=True)\n",
        "\n",
        "class DeviceDataloader():\n",
        "  def __init__(self,device,dl):\n",
        "    self.dl=dl\n",
        "    self.device=device\n",
        "\n",
        "  def __iter__(self):\n",
        "    for batch in self.dl:\n",
        "      yield to_device(batch,self.device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dl)\n",
        "\n",
        "device=get_default_device()\n",
        "\n",
        "train_dataloaders = []\n",
        "for video_dataset in train_dataset:\n",
        "    # Shuffle=False ensures we process frames in order\n",
        "    video_dataloader = DataLoader(video_dataset, batch_size=128, shuffle=False,\n",
        "                                  num_workers=2, pin_memory=True, drop_last=False)\n",
        "    video_dataloader = DeviceDataloader(device, video_dataloader)\n",
        "    train_dataloaders.append(video_dataloader)\n",
        "\n",
        "print(\"Training datasets loaded...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4-ypNgD2oBC7"
      },
      "outputs": [],
      "source": [
        "tool_weight     = [0.93487068, 0.94234964, 0.93487068, 1.18448115, 1.02368339, 0.97974447]\n",
        "verb_weight     = [0.60002400, 0.60002400, 0.60002400, 0.61682467, 0.67082683, 0.80163207, 0.70562823, 2.11208448, 2.69230769, 0.60062402]\n",
        "target_weight   = [0.49752894, 0.52041527, 0.49752894, 0.51394739, 2.71899565, 1.75577963, 0.58509403, 1.25228034, 0.49752894, 2.42993134, 0.49802647, 0.87266576, 1.36074165, 0.50150917, 0.49802647]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AsoPzNk6Rab"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Rendezvous().to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "miuRdEtJ7AUk"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "benchmark   = torch.nn.Parameter(torch.tensor([0.0]), requires_grad=False)\n",
        "print(\"Model built ...\")\n",
        "# Loss\n",
        "activation  = nn.Sigmoid()\n",
        "loss_fn_i   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(tool_weight).to(device))\n",
        "loss_fn_v   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(verb_weight).to(device))\n",
        "loss_fn_t   = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(target_weight).to(device))\n",
        "loss_fn_ivt = nn.BCEWithLogitsLoss()\n",
        "loss_fn_p   = nn.BCEWithLogitsLoss()\n",
        "\n",
        "# evaluation metrics\n",
        "mAP = ivtmetrics.Recognition(100)\n",
        "mAP.reset_global()\n",
        "mAPi = ivtmetrics.Recognition(6)\n",
        "mAPv = ivtmetrics.Recognition(10)\n",
        "mAPt = ivtmetrics.Recognition(15)\n",
        "mAPi.reset_global()\n",
        "mAPv.reset_global()\n",
        "mAPt.reset_global()\n",
        "print(\"Metrics built ...\")\n",
        "\n",
        "opt=torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "steps_per_epoch=0\n",
        "for dl in train_dataloaders:\n",
        "  steps_per_epoch+=len(dl)\n",
        "scheduler = OneCycleLR(opt, max_lr=1e-3, epochs=100, steps_per_epoch=steps_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RMaMg2x6_DF0"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "for epoch in range(0,50):\n",
        "        # Train\n",
        "        train_video_wise(train_dataloaders, model, opt, activation, scheduler, loss_fn_p, loss_fn_i, loss_fn_v, loss_fn_t, loss_fn_ivt)\n",
        "\n",
        "        if ((epoch+1)%5 == 0):\n",
        "          torch.save(model.state_dict(), f\"/content/drive/MyDrive/BH25/try1/{epoch+1}.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW2NcUDgEN0R"
      },
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pl4FgFS6ESEh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
        "import sys\n",
        "import torch\n",
        "import platform\n",
        "import ivtmetrics\n",
        "from torch import nn\n",
        "import json\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zy2IPK3JEemV"
      },
      "outputs": [],
      "source": [
        "class T50(Dataset):\n",
        "    def __init__(self, img_dir, triplet_file, tool_file, verb_file, target_file, phase_file, transform=None, target_transform=None):\n",
        "        self.triplet_labels = np.loadtxt(triplet_file, dtype=int, delimiter=',')\n",
        "        self.tool_labels = np.loadtxt(tool_file, dtype=int, delimiter=',')\n",
        "        self.verb_labels = np.loadtxt(verb_file, dtype=int, delimiter=',')\n",
        "        self.target_labels = np.loadtxt(target_file, dtype=int, delimiter=',')\n",
        "        self.phase_labels = np.loadtxt(phase_file, dtype=int, delimiter=',')\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.triplet_labels)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        triplet_label = self.triplet_labels[index, 1:]\n",
        "        tool_label = self.tool_labels[index, 1:]\n",
        "        verb_label = self.verb_labels[index, 1:]\n",
        "        target_label = self.target_labels[index, 1:]\n",
        "        phase_label = self.phase_labels[index, 1:]\n",
        "        basename = \"{}.png\".format(str(self.triplet_labels[index, 0]).zfill(6))\n",
        "        img_path = os.path.join(self.img_dir, basename)\n",
        "        image    = Image.open(img_path)\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            triplet_label = self.target_transform(triplet_label)\n",
        "        return str(self.triplet_labels[index, 0]).zfill(6), image, (tool_label, verb_label, target_label, triplet_label, phase_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7iRgzHhEltu"
      },
      "outputs": [],
      "source": [
        "class CholecT50():\n",
        "    def __init__(self,\n",
        "                dataset_dir):\n",
        "      self.dataset_dir = dataset_dir\n",
        "      train_videos = [1, 2, 4, 5, 6, 8, 10, 12, 13, 14]\n",
        "      test_videos = [92, 96, 103, 110, 111]\n",
        "      self.train_records = ['VID{}'.format(str(v).zfill(2)) for v in train_videos]\n",
        "      self.test_records  = ['VID{}'.format(str(v).zfill(2)) for v in test_videos]\n",
        "      trainform, testform = self.transform()\n",
        "      self.build_train_dataset(trainform)\n",
        "      self.build_test_dataset(testform)\n",
        "\n",
        "    def no_augumentation(self, x):\n",
        "      return x\n",
        "\n",
        "    def transform(self):\n",
        "      normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "      op_test   = [transforms.Resize((256, 448)), transforms.ToTensor(), normalize]\n",
        "      op_train  = [transforms.Resize((256, 448)), transforms.ToTensor(), normalize]\n",
        "      testform  = transforms.Compose(op_test)\n",
        "      trainform = transforms.Compose(op_train)\n",
        "      return trainform, testform\n",
        "\n",
        "    def add_black_frames_and_zeros(self,video_path, txt_files, img_shape=(256, 448)):\n",
        "      # Step 1: Add 9 black frames\n",
        "      black_frame = np.zeros((256, 448, 3), dtype=np.uint8)\n",
        "      for i in range(9):\n",
        "          black_frame_img = Image.fromarray(black_frame)\n",
        "          save_path=os.path.join(video_path, f\"{i+1}0000.png\")\n",
        "          black_frame_img.save(save_path)\n",
        "\n",
        "      # Step 2: Add 9 lines of zeros to each text file\n",
        "      for txt_file in txt_files:\n",
        "          with open(txt_file, 'r') as file:\n",
        "              lines = file.readlines()\n",
        "\n",
        "          zero_line = [0 for _ in range(len(lines[0].split(\",\"))-1)]  # Assuming all lines have the same number of entries\n",
        "\n",
        "          for i in range(9):\n",
        "            output_line = f\"{i+1}0000,\" + \",\".join(map(str, zero_line))\n",
        "            final_line = f\"{output_line}\\n\"\n",
        "            lines.insert(0, final_line)\n",
        "\n",
        "          with open(txt_file, 'w') as file:\n",
        "              file.writelines(lines)\n",
        "\n",
        "    def build_train_dataset(self, transform):\n",
        "        iterable_dataset = []\n",
        "        for video in self.train_records:\n",
        "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'data', video),\n",
        "                        triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video)),\n",
        "                        tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video)),\n",
        "                        verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video)),\n",
        "                        target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video)),\n",
        "                        phase_file =  os.path.join(self.dataset_dir, 'phase', '{}.txt'.format(video)),\n",
        "                        transform=transform)\n",
        "            iterable_dataset.append(dataset)\n",
        "        self.train_dataset = iterable_dataset\n",
        "\n",
        "    def build_test_dataset(self, transform):\n",
        "        iterable_dataset = []\n",
        "        for video in self.test_records:\n",
        "            img_dir = os.path.join(self.dataset_dir, 'data', video)\n",
        "            print(img_dir)\n",
        "            triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video))\n",
        "            tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video))\n",
        "            verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video))\n",
        "            target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video))\n",
        "            phase_file = os.path.join(self.dataset_dir, 'phase', '{}.txt'.format(video))\n",
        "\n",
        "            # Add 9 black frames and zeros to text files\n",
        "            self.add_black_frames_and_zeros(img_dir, [triplet_file, tool_file, verb_file, target_file, phase_file])\n",
        "\n",
        "            dataset = T50(img_dir = os.path.join(self.dataset_dir, 'data', video),\n",
        "                triplet_file = os.path.join(self.dataset_dir, 'triplet', '{}.txt'.format(video)),\n",
        "                tool_file = os.path.join(self.dataset_dir, 'instrument', '{}.txt'.format(video)),\n",
        "                verb_file = os.path.join(self.dataset_dir, 'verb', '{}.txt'.format(video)),\n",
        "                target_file = os.path.join(self.dataset_dir, 'target', '{}.txt'.format(video)),\n",
        "                phase_file = os.path.join(self.dataset_dir, 'phase', '{}.txt'.format(video)),\n",
        "                transform=transform)\n",
        "            iterable_dataset.append(dataset)\n",
        "        self.test_dataset = iterable_dataset\n",
        "\n",
        "    def build(self):\n",
        "        return (self.train_dataset, self.test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4RXxiAHEsTU"
      },
      "outputs": [],
      "source": [
        "!pip install ivtmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0LVoCOUE8Du"
      },
      "outputs": [],
      "source": [
        "def test_loop(num,dataloader, model, activation, final_eval=False):\n",
        "    #size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "\n",
        "    #Dictionary of tools for each triplet(triplet key tool value)\n",
        "\n",
        "    l = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0,10:0,11:0,12:0,13:0,14:0,15:0,16:0,17:0,18:0,19:0,20:0,21:0,22:1,23:1,24:1,25:1,26:1,27:1,28:1,29:1,30:1,31:1,32:1,33:1,34:1,35:1,\n",
        "    36:1,37:1,38:1,39:1,40:1,41:1,42:1,43:1,44:1,45:1,46:2,47:2,48:2,49:2,50:2,51:2,52:2,53:2,54:2,55:2,56:2,57:2,58:2,59:2,60:2,61:2,62:2,63:2,64:2,65:3,66:3,67:3,68:3,69:3,70:3,\n",
        "    71:3,72:3,73:3,74:3,75:3,76:3,77:4,78:4,79:4,80:4,81:4,82:5,83:5,84:5,85:5,86:5,87:5,88:5,89:5,90:5,91:5,92:5,93:5,94:0,95:1,96:2,97:3,98:4,99:5}\n",
        "\n",
        "    json_data = {}\n",
        "    last_9_imgs=[]\n",
        "    batch_num=1\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch, (frame_ids, img, (y1, y2, y3, y4, y5)) in enumerate(dataloader):\n",
        "            model.eval()\n",
        "\n",
        "            if(batch_num==1):\n",
        "              batch_num+=1\n",
        "              frame_ids = frame_ids[9:]\n",
        "            else:\n",
        "              img = torch.cat((last_9_imgs, img), dim=0)\n",
        "\n",
        "            if (img.shape[0]<10):\n",
        "              continue\n",
        "            last_9_imgs = img[-9:]\n",
        "            tool, verb, target, triplet, phase = model(img)\n",
        "            cam_i, logit_i = tool\n",
        "            cam_v, logit_v = verb\n",
        "            cam_t, logit_t = target\n",
        "            logit_phase    = phase\n",
        "\n",
        "            # Convert logits to probabilities\n",
        "\n",
        "            probabilities = F.softmax(triplet, dim=-1)\n",
        "            triplets = probabilities.cpu().numpy()\n",
        "            for frame_idx, frame_id in enumerate(frame_ids):\n",
        "\n",
        "              curr_cam_i = cam_i[frame_idx]\n",
        "              curr_logit_i = logit_i[frame_idx]\n",
        "              curr_triplet = triplets[frame_idx]\n",
        "              # Calculate probabilities from logits\n",
        "              tool_probs = F.softmax(curr_logit_i, dim=-1).cpu().numpy()\n",
        "\n",
        "              # Structure detection entries\n",
        "              detection_entries = []\n",
        "              for trip_id, trip_prob in enumerate(curr_triplet):\n",
        "                  tool_id = l[trip_id]\n",
        "                  tool_prob = tool_probs[tool_id]\n",
        "                  curr_cam = curr_cam_i[tool_id]\n",
        "                  curr_cam = curr_cam.unsqueeze(0)\n",
        "                  probabilities = F.softmax(curr_cam_i, dim=(0))\n",
        "                  cam = curr_cam.squeeze(0)\n",
        "                  mean_val = cam.mean().item()\n",
        "                  std_dev = cam.std().item()\n",
        "                  threshold = mean_val + 2 * std_dev\n",
        "                  binary_mask = (cam >= threshold).float()\n",
        "\n",
        "                  non_zero_indices = torch.nonzero(binary_mask)\n",
        "                  if non_zero_indices.numel() == 0:\n",
        "                    x_left = torch.tensor([-1])\n",
        "                    y_top  = torch.tensor([-1])\n",
        "                    width  = torch.tensor([-1])\n",
        "                    height = torch.tensor([-1])\n",
        "                  else:\n",
        "                    y_top, x_left = torch.min(non_zero_indices, dim=0).values\n",
        "                    y_bottom, x_right = torch.max(non_zero_indices, dim=0).values\n",
        "                    width = x_right-x_left\n",
        "                    height = y_bottom-y_top\n",
        "\n",
        "                  bbox_x, bbox_y, bbox_w, bbox_h = x_left.item()/14, y_top.item()/8, width.item()/14, height.item()/8\n",
        "                  detection_entry = {\n",
        "                    \"triplet\": int(trip_id),\n",
        "                    \"instrument\": [int(tool_id), float(tool_prob), float(abs(bbox_x)), float(abs(bbox_y)), float(abs(bbox_w)), float(abs(bbox_h))]\n",
        "                  }\n",
        "                  detection_entries.append(detection_entry)\n",
        "\n",
        "              # Structure JSON data for each frame\n",
        "              json_data[str(frame_id)] = {\n",
        "                  \"recognition\": curr_triplet.tolist(),  # Probabilities for each triplet\n",
        "                  \"detection\": detection_entries\n",
        "              }\n",
        "\n",
        "        output_path=f\"/content/drive/MyDrive/BH25/try1/output{num}.json\"\n",
        "        # Save to JSON file\n",
        "        with open(output_path, 'w') as json_file:\n",
        "            json.dump(json_data, json_file, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j038syexFDrq"
      },
      "outputs": [],
      "source": [
        "dataset = CholecT50(dataset_dir=\"/content/drive/MyDrive/CholecT50_unzipped/CholecT50\")\n",
        "\n",
        "# # build dataset\n",
        "train_dataset, test_dataset = dataset.build()\n",
        "\n",
        "def get_default_device():\n",
        "  if torch.cuda.is_available():\n",
        "    return torch.device('cuda')\n",
        "  else:\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def to_device(data,device):\n",
        "  if isinstance(data,(list,tuple)):\n",
        "    return [to_device(x,device) for x in data]\n",
        "  elif isinstance(data,str):\n",
        "    return data\n",
        "  else:\n",
        "    return data.to(device,non_blocking=True)\n",
        "\n",
        "class DeviceDataloader():\n",
        "  def __init__(self,device,dl):\n",
        "    self.dl=dl\n",
        "    self.device=device\n",
        "\n",
        "  def __iter__(self):\n",
        "    for batch in self.dl:\n",
        "      yield to_device(batch,self.device)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.dl)\n",
        "\n",
        "device=get_default_device()\n",
        "\n",
        "test_dataloaders = []\n",
        "for video_dataset in test_dataset:\n",
        "    test_dataloader = DataLoader(video_dataset, batch_size=128, shuffle=False, num_workers=2, pin_memory=True, drop_last=False)\n",
        "    test_dataloader = DeviceDataloader(device, test_dataloader)\n",
        "    test_dataloaders.append(test_dataloader)\n",
        "print(\"Test Dataset loaded ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCW5FSz8FMdH"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Rendezvous().to(device)\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDoViCaEFU2R"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "benchmark   = torch.nn.Parameter(torch.tensor([0.0]), requires_grad=False)\n",
        "print(\"Model built ...\")\n",
        "# Loss\n",
        "activation  = nn.Sigmoid()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5liEUFhFaTm"
      },
      "outputs": [],
      "source": [
        "# Final testing loop\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/BH25/try1/50.pt\"))# No need if testing in same session as training\n",
        "#mAP.reset_global()\n",
        "num=1\n",
        "for test_dataloader in test_dataloaders:\n",
        "    test_loop(num,test_dataloader, model, activation, final_eval=True)\n",
        "    print(f\"Video{num} done\")\n",
        "    num+=1\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
